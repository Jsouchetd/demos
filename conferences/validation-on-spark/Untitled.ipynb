{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4b491c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandera\n",
    "import fugue\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark\n",
    "from great_expectations.dataset.sparkdf_dataset import SparkDFDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "494c5c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62a951f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.randint(1, 100, 1000)\n",
    "b = np.random.randint(1, 100, 1000)\n",
    "test1 = pd.DataFrame({'a':a, 'b':b})\n",
    "test2 = test1 + 100\n",
    "test1['partition'] = 'a'\n",
    "test2['partition'] = 'b'\n",
    "test = test1.append(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e95bc6ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"success\": false,\n",
       "  \"exception_info\": {\n",
       "    \"raised_exception\": false,\n",
       "    \"exception_traceback\": null,\n",
       "    \"exception_message\": null\n",
       "  },\n",
       "  \"meta\": {},\n",
       "  \"result\": {\n",
       "    \"element_count\": 2000,\n",
       "    \"missing_count\": 0,\n",
       "    \"missing_percent\": 0.0,\n",
       "    \"unexpected_count\": 1037,\n",
       "    \"unexpected_percent\": 51.849999999999994,\n",
       "    \"unexpected_percent_total\": 51.849999999999994,\n",
       "    \"unexpected_percent_nonmissing\": 51.849999999999994,\n",
       "    \"partial_unexpected_list\": [\n",
       "      96,\n",
       "      98,\n",
       "      96,\n",
       "      97,\n",
       "      99,\n",
       "      98,\n",
       "      96,\n",
       "      96,\n",
       "      98,\n",
       "      96,\n",
       "      96,\n",
       "      98,\n",
       "      97,\n",
       "      98,\n",
       "      97,\n",
       "      98,\n",
       "      98,\n",
       "      97,\n",
       "      98,\n",
       "      98\n",
       "    ],\n",
       "    \"partial_unexpected_index_list\": null,\n",
       "    \"partial_unexpected_counts\": [\n",
       "      {\n",
       "        \"value\": 98,\n",
       "        \"count\": 9\n",
       "      },\n",
       "      {\n",
       "        \"value\": 96,\n",
       "        \"count\": 6\n",
       "      },\n",
       "      {\n",
       "        \"value\": 97,\n",
       "        \"count\": 4\n",
       "      },\n",
       "      {\n",
       "        \"value\": 99,\n",
       "        \"count\": 1\n",
       "      }\n",
       "    ]\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_test = spark.createDataFrame(test)\n",
    "spark_test = spark_test.withColumnRenamed(\"a\",\"col1\")\n",
    "sparkdf = SparkDFDataset(spark_test)\n",
    "sparkdf.expect_column_values_to_be_between(\"col1\", \n",
    "                                            min_value=0, \n",
    "                                            max_value=9, \n",
    "                                            mostly=0.95,\n",
    "                                            result_format=\"SUMMARY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f47d5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # schema: *, filled:double\n",
    "# def fillna(df:Iterable[Dict[str,Any]], value:float=0) -> Iterable[Dict[str,Any]]:\n",
    "#     for row in df:\n",
    "#         row[\"filled\"] = (row[\"value\"] or value)\n",
    "#         yield row\n",
    "\n",
    "# with FugueWorkflow(SparkExecutionEngine) as dag:\n",
    "#     df = dag.load(\"file.parquet\").transform(fillna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713510ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from great_expectations.dataset.sparkdf_dataset import SparkDFDataset\n",
    "\n",
    "sparkdf = SparkDFDataset(sparkdf)\n",
    "\n",
    "sparkdf.expect_column_values_to_be_between(\"col1\", \n",
    "                                            min_value=0, \n",
    "                                            max_value=95, \n",
    "                                            mostly=0.95,\n",
    "                                            result_format=\"SUMMARY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31e61fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandera as pa\n",
    "from pandera import Column, Check, DataFrameSchema\n",
    "\n",
    "price_check = pa.DataFrameSchema({\n",
    "    \"price\": Column(pa.Float, Check.in_range(min_value=5,max_value=10)),\n",
    "})\n",
    "price_check.validate(df)\n",
    "\n",
    "# schema: *\n",
    "def price_validation(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    price_check.validate(df)\n",
    "    return df\n",
    "\n",
    "with FugueWorkflow(SparkExecutionEngine) as dag:\n",
    "    df = df.transform(price_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408c948f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandera as pa\n",
    "from pandera import Column, Check, DataFrameSchema\n",
    "\n",
    "price_check_FL = pa.DataFrameSchema({\n",
    "    \"price\": Column(pa.Float, Check.in_range(min_value=5,max_value=10)),\n",
    "})\n",
    "\n",
    "price_check_CA = pa.DataFrameSchema({\n",
    "    \"price\": Column(pa.Float, Check.in_range(min_value=10,max_value=15)),\n",
    "})\n",
    "\n",
    "price_checks = {'CA': price_check_CA, 'FL': price_check_FL}\n",
    "\n",
    "# schema: *\n",
    "def price_validation(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    location = df['location'].iloc[0]\n",
    "    check = price_checks[location]\n",
    "    check.validate(df)\n",
    "    return df\n",
    "\n",
    "with FugueWorkflow(SparkExecutionEngine) as dag:\n",
    "    df = df.partition(by=[\"location\"]).transform(price_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe0935c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandera as pa\n",
    "from pandera import Column, Check, DataFrameSchema\n",
    "\n",
    "schema_test1 = pa.DataFrameSchema({\n",
    "    \"a\": Column(pa.Int, Check.is_b(100)),\n",
    "})\n",
    "\n",
    "schema_test2 = pa.DataFrameSchema({\n",
    "    \"a\": Column(pa.Int, Check.greater_than(99))\n",
    "})\n",
    "\n",
    "partition_schema = {\"a\": schema_test1, \"b\": schema_test2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0b62e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# schema: *\n",
    "def validator(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    partition = df['partition'].iloc[0]\n",
    "    schema = partition_schema[partition]\n",
    "    schema.validate(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68013ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue import FugueWorkflow\n",
    "from fugue_spark import SparkExecutionEngine\n",
    "\n",
    "with FugueWorkflow(SparkExecutionEngine) as dag:\n",
    "    df = dag.df(test)\n",
    "    df = df.partition(by=[\"partition\"]).transform(validator)\n",
    "    df.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
