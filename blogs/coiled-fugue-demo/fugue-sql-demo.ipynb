{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis with FugueSQL on Coiled Dask Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will discuss [fugue-sql](https://docs.dask.org/en/latest/dataframe-sql.html#does-dask-implement-sql), a SQL abstraction layer that allows users to run SQL queries on top of Pandas, Spark, and Dask DataFrames. fugue-sql is part of the broader [fugue project](https://github.com/fugue-project/fugue), which aims to be an abstaction layer for distributed compute workflows. Fugue has both a Python and SQL interface. Users can choose the execution engine (Pandas, Spark, Dask) to run their logic on just by specifying during runtime.\n",
    "\n",
    "FugueSQL is meant for data analysts and SQL lovers to harness the power of distributed compute using a fun, and more English-like, syntax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/fugue-project/fugue/master/images/logo.svg\" align=\"left\" width=\"250\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libaries\n",
    "Here we import some standard data science libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import coiled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Software Environment\n",
    "\n",
    "The commented out code below was used to create the software environment for our cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating software environment...\n",
      "Using existing conda solve...\n",
      "Building Docker image\n",
      "Found built image for dev/kvnkho-fugue-sql:5187c038-d943-49e6-a7b4-dcba7f4f5ce8!\n",
      "Finished updating environment\n"
     ]
    }
   ],
   "source": [
    "coiled.create_software_environment(\n",
    "    name=\"fugue-sql\",\n",
    "    conda={\"channels\": [\"conda-forge\"],\n",
    "            \"dependencies\": [\"dask=2021.04.0\",\"distributed=2021.04.0\",\"python=3.7.9\",\"s3fs\"]},\n",
    "    pip=['fugue']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Coiled Cluster\n",
    "\n",
    "We need to create a Coiled cluster with a predefined image. The image contains fugue and some fugue dependencies. If you're doing this in your Coiled account, don't forget to change the software to your account. Instead of `kvnkho/fugue-sql`, it will look like `username/fugue-sql` if the software environment above was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a252c90da35a401090f44449d59ecbb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking environment images\n",
      "Valid environment image found\n"
     ]
    }
   ],
   "source": [
    "# Creating the Coiled cluster\n",
    "cluster = coiled.Cluster(\n",
    "    n_workers=2,\n",
    "    software=\"kvnkho/fugue-sql\",\n",
    ")\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we pass the cluster to our Dask Client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "`fugue-sql` can be imported in notebooks by using the `fugue_notebook.setup` function. This provides syntax highlighting for fugue-sql cells and allows us to use the `%%fsql` magic.\n",
    "\n",
    "At the moment, the syntax highlighting is only available for traditional iPython notebooks. It will fail in JupyterLab environments.\n",
    "\n",
    "To open the classic iPython Notebook from JupyterLab, select “Launch Classic Notebook” from the JupyterLab Help menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue_notebook import setup\n",
    "try:\n",
    "    setup()\n",
    "except:\n",
    "    print(\"fugue-sql cell magic will work but syntax highlighting is not yet available for JupyterLab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Look at fugue-sql\n",
    "\n",
    "Here we load in the NYC taxi data with dask. This is a standard dataset frequently used in other Dask demos. Persist prevents re-reading unnecessarily. Some of these columns hold NULL values, so we need to store it float because of compatibility with PyArrow. The integer data type can't hold NULL values in PyArrow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_df = dd.read_csv(\n",
    "    \"s3://nyc-tlc/trip data/yellow_tripdata_2019-01.csv\",\n",
    "    dtype={'RatecodeID': 'float64',\n",
    "           'VendorID': 'float64',\n",
    "           'passenger_count': 'float64',\n",
    "           'payment_type': 'float64'},\n",
    "    storage_options={\"anon\": True},\n",
    "    blocksize=\"16 MiB\",\n",
    ").persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell is our first look at fugue-sql. Notice the `%%fsql` cell magic turns the whole cell into a SQL cell. `%%fsql dask` tells Fugue to use Dask as an execution engine. Not specifying an execution engine defaults to pandas.\n",
    "\n",
    "The Dask DataFrame (named `taxi_df`) that was previously loaded in is now accessible by this SQL cell. `fugue-sql` has all ANSI SQL keywords available, so here we show a simple `GROUP BY` and `ORDER BY` to get the average tip by number of passengers. We also use the `AVG` aggregate function.\n",
    "\n",
    "For iPython notebook users, syntax highlighting for SQL keywords will be available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%fsql dask\n",
    "-- This is a SQL cell\n",
    "tempdf = SELECT passenger_count, AVG(tip_amount) AS average_tip\n",
    "           FROM taxi_df\n",
    "       GROUP BY passenger_count\n",
    "\n",
    "  SELECT *\n",
    "    FROM tempdf\n",
    "ORDER BY passenger_count ASC\n",
    "   LIMIT 5\n",
    "   PRINT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics of fugue-sql\n",
    "\n",
    "fugue-sql aims to make SQL easier to use. To support end-to-end workflows, some enhancements have been added. One of them is already visible above where we assigned a query to `tempdf` . This is similar to the SQL temp tables and common table expressions (CTEs)\n",
    "\n",
    "For familiar Dask users, this is a delayed execution that runs when `PRINT` is called. This happens because `fugue-sql` uses Dask, which constructs a Directed Acyclic Graph (DAG) to perform the operations lazily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing Python DataFrames to fugue-sql\n",
    "\n",
    "`fugue-sql` supports Python interoperatibility. DataFrames defined outside `%%fsql` cells can be used inside the SQL queries. In this example, we create an example DataFrame and use it inside a following `fugue-sql` code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = pd.DataFrame({'a':[1,2,3],'b':[1,2,3]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passing fugue-sql results to Python\n",
    "\n",
    "By default, the DataFrames inside fugue-sql cells **will not be accessible** by Python cells (or even by succeeding fugue-sql cells). We have to use the `YIELD DATAFRAME` keyword to make a DataFrame available in memory. For significantly large DataFrames, we can use the `YIELD FILE` keyword. This saves the file in a temporary location and loads it when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%fsql\n",
    "SELECT * \n",
    "  FROM example\n",
    " WHERE a > 1\n",
    " YIELD DATAFRAME AS filtered_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%fsql\n",
    "-- Here we can use the YIELDED dataframe\n",
    "SELECT *\n",
    "  FROM filtered_example\n",
    " PRINT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, we use the same `YIELDED` DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using a YIELDED dataframe\n",
    "filtered_example.as_pandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Save\n",
    "\n",
    "`fugue-sql`allows users to `LOAD` from csv/json/parquet files using Pandas and Dask under the hood. This means we can load in data, perform transformations on it, and then `SAVE` the results. Data analysts can now work on flat files with a language they are comfortable in. The cell below runs in Pandas because we don't specify the engine after `%%fsql`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%fsql\n",
    "SELECT * FROM example\n",
    "SAVE OVERWRITE \"/tmp/f.csv\" (header=TRUE)\n",
    "\n",
    "loaded_example = LOAD \"/tmp/f.csv\" (header=TRUE)\n",
    "PRINT "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jinja Templating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes a Python variable will be needed inside a SQL block. Think of dynamic lists used to filter values in a DataFrame. In this case, Jinja templating can be used to pass a variable inside a fsql code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a Python cell\n",
    "n = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%fsql\n",
    "SELECT *\n",
    "  FROM example\n",
    " WHERE a = {{n}}\n",
    " PRINT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Altering Schema\n",
    "\n",
    "Note that if we don't infer the schema, Pandas loads most columns as strings. We can use `ALTER COLUMNS` to change the schema. For DataFrames with a large number of columns, we recommend using `infer_schema=TRUE` and then `ALTER COLUMNS` to ensure the correct types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%fsql\n",
    "df = LOAD \"/tmp/f.csv\" (header = TRUE, infer_schema=TRUE)\n",
    "df = ALTER COLUMNS a:int, b:str FROM df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anonymity and Inline\n",
    "\n",
    "Anonymity is when the DataFrame to perform the operation on is not specified. As a default, the output of the last operation will be used. This is a `fugue-sql` feature designed to simplify code. `PRINT` is an example of this.\n",
    "\n",
    "Inline statements are wrapping another SQL statement inside parenthesis so that they are evaluated first as part of an outer SQL statement. This is another option instead of assigning DataFrames into variables. Notice the `LOAD` statement below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%fsql\n",
    "df = SELECT * FROM (LOAD \"/tmp/f.csv\" (header=TRUE))\n",
    "ALTER COLUMNS a:int, b:double\n",
    "PRINT 5 ROWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data Analysis\n",
    "\n",
    "Now that we have covered the basics, we will perform a simple analysis and show some more advance features such as integration Python code and utilizing the Dask execution engine more. First we start by displaying the head of our DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%fsql dask\n",
    "SELECT *\n",
    "  FROM taxi_df\n",
    " LIMIT 10 \n",
    " PRINT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Python Functions\n",
    "\n",
    "Here we will demonstrate using Python functions together with fugue-sql. Below we define a simple function that outputs a plot with the given `x_col` and `y_col`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seaborn_barplot(df: pd.DataFrame, x_col:str, y_col:str) -> None:\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.barplot(df[x_col].astype('str').fillna(\"None\"), df[y_col])\n",
    "    plt.ylabel(y_col, fontsize=12)\n",
    "    plt.xlabel(x_col, fontsize=12)\n",
    "    plt.title(f\"{x_col} vs {y_col}\", fontsize=15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use this function with the `OUTPUT` keyword. The `OUTPUT` keyword is for functions that don't return anything. To avoid passing a lot of data into seaborn, we will `SAMPLE` 10 percent of the data. This is an additional` fugue-sql` keyword. Also note the use of anonymity to use the Python function multiple times on the same DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%fsql dask\n",
    "SELECT payment_type, total_amount, congestion_surcharge, improvement_surcharge\n",
    "  FROM taxi_df\n",
    "SAMPLE 10 PERCENT SEED 42\n",
    "OUTPUT USING seaborn_barplot PARAMS (x_col:\"payment_type\", y_col:\"total_amount\")\n",
    "OUTPUT USING seaborn_barplot PARAMS (x_col: \"congestion_surcharge\", y_col:\"total_amount\")\n",
    "OUTPUT USING seaborn_barplot PARAMS (x_col: \"improvement_surcharge\", y_col:\"total_amount\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control Over Partitions\n",
    "\n",
    "In the previous plots, there is a weird relationship between **improvement_surcharge** and **total_amount** paid when **improvement_surcharge==0**. We can explore this by looking at the rows with high **total_amount** values.\n",
    "\n",
    "The `TAKE` keyword lets us return the whole row. We first `PREPARTITION` our data by **improvement_surcharge** values and then get the rows with the top 3 **total_amount** values. fugue-sql allows users to control the partitioning of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%fsql dask\n",
    "SELECT trip_distance, passenger_count, total_amount, improvement_surcharge\n",
    "  FROM taxi_df\n",
    "  TAKE 3 ROWS PREPARTITION BY improvement_surcharge PRESORT total_amount DESC\n",
    " PRINT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Custom Python Functions\n",
    "\n",
    "The previous table hints that there are uneven counts for each `improvement_surcharge` value. We can explore this by using a custom Python function\n",
    "\n",
    "We define the `value_counts` function below. Note that the schema hint comment is read and enforced to ensure that the output of the function matches the specified schema. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# schema: value:str, count:int\n",
    "def value_counts(df: pd.DataFrame, col: str) -> pd.DataFrame:\n",
    "    out = df[[col]].value_counts().reset_index()\n",
    "    out.columns = ['value', 'count']\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For functions that return another DataFrame, we use the `TRANSFORM` keyword. The output may be a bit different than expected. Note that we have multiple rows for the same `value`. This is because the function was applied over the default partitions, and returned the output for each partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%fsql dask\n",
    "SELECT * FROM taxi_df\n",
    "TRANSFORM USING value_counts PARAMS(col:\"improvement_surcharge\")\n",
    "PRINT 6 ROWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to fix the behavrior above, we need to specify the partitioning scheme before the operation is done. This can be specified within the `TRANSFORM` statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%fsql dask\n",
    "SELECT * FROM taxi_df\n",
    "TRANSFORM PREPARTITION BY improvement_surcharge \n",
    "USING value_counts PARAMS(col:\"improvement_surcharge\")\n",
    "PRINT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPARTITION and TRANSFORM\n",
    "\n",
    "Partitioning is a very important distributed computing concept. Because data lives on different machines in a distributed environment, some operations require all data in the same group to be collected on the same machine (sometimes even in a specific order). The most common example is getting the median value for each group. Another frequest use case is collecting timeseries by group and in order (think stock data). \n",
    "\n",
    "The `PREPARTITION` keyword of `fugue-sql` allows users to control the partitioning scheme. The `TRANSFORM` keyword lets users apply Python functions through `fugue-sql`. Together these two keywords build a semantic similar to the `groupby-apply` semantic in dask and make a lot of operations easier than using pure SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment\n",
    "\n",
    "When ready to move from notebooks to Python scripts, the `fsql()` class can be used. The `run` method takes in the execution engine. If none is provided, Pandas is used. This is the same query used in the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue_sql import fsql\n",
    "fsql(\"\"\"SELECT * \n",
    "        FROM taxi_df\n",
    "        TRANSFORM PREPARTITION BY improvement_surcharge \n",
    "        USING value_counts PARAMS(col:\"improvement_surcharge\")\n",
    "        PRINT\"\"\").run(\"dask\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Here we have shown an example of using `fugue-sql` with the Dask execution engine. `fugue-sql` is a language to express computation workflows through SQL. The expressed logic can then be run on Pandas, Dask or Spark, without having to change the code. There are enhancements over standard SQL that allow it to be used for full end-to-end workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More information\n",
    "\n",
    "We have only scratched the surface of `fugue` and `fugue-sql`. `fugue` is a pure abstraction layer that makes code portable across differing computing frameworks such as Pandas, Spark and Dask. It decouples logic from execution engines, allowing users to write code without worrying about the execution engine. All questions about `fugue` and distributed compute are welcome in the `fugue` Slack channel.\n",
    "\n",
    "[Fugue Repo](https://github.com/fugue-project/fugue)\n",
    "\n",
    "[Fugue Slack](https://join.slack.com/t/fugue-project/shared_invite/zt-jl0pcahu-KdlSOgi~fP50TZWmNxdWYQ)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
